#!/usr/bin/env python3
"""
An√°lise pontual: Relat√≥rio completo do projeto dengue_prediction_fabio_kon
An√°lise de todos os c√≥digos e funcionalidades implementadas
"""

import os
from pathlib import Path
import ast
import re

def analisar_projeto_fabio_kon():
    """
    An√°lise completa do projeto de predi√ß√£o de dengue do Fabio Kon
    """
    
    print("="*80)
    print(" RELAT√ìRIO: PROJETO DENGUE_PREDICTION_FABIO_KON ".center(80, "="))
    print("="*80)
    
    base_dir = Path(__file__).parents[2]
    projeto_dir = base_dir / "dengue_prediction_fabio_kon"
    
    if not projeto_dir.exists():
        print(f"‚ùå Pasta do projeto n√£o encontrada!")
        return
    
    print(f"üìÅ Analisando projeto em: {projeto_dir}")
    
    # 1. ESTRUTURA GERAL DO PROJETO
    print(f"\n{'-'*60}")
    print("1. ESTRUTURA DO PROJETO")
    print(f"{'-'*60}")
    
    arquivos = list(projeto_dir.glob("*"))
    arquivos_python = [f for f in arquivos if f.suffix == '.py']
    outros_arquivos = [f for f in arquivos if f.suffix != '.py' and f.is_file()]
    pastas = [f for f in arquivos if f.is_dir()]
    
    print(f"\nüìä COMPOSI√á√ÉO:")
    print(f"   ‚Ä¢ Arquivos Python: {len(arquivos_python)}")
    print(f"   ‚Ä¢ Outros arquivos: {len(outros_arquivos)}")
    print(f"   ‚Ä¢ Pastas: {len(pastas)}")
    
    print(f"\nüìù ARQUIVOS PYTHON:")
    for arquivo in sorted(arquivos_python):
        tamanho = arquivo.stat().st_size / 1024  # KB
        print(f"   ‚Ä¢ {arquivo.name:<25} ({tamanho:.1f} KB)")
    
    print(f"\nüìÑ OUTROS ARQUIVOS:")
    for arquivo in sorted(outros_arquivos):
        tamanho = arquivo.stat().st_size / 1024  # KB
        print(f"   ‚Ä¢ {arquivo.name:<25} ({tamanho:.1f} KB)")
    
    # 2. AN√ÅLISE DO README
    print(f"\n{'-'*60}")
    print("2. DOCUMENTA√á√ÉO (README.md)")
    print(f"{'-'*60}")
    
    readme_file = projeto_dir / "README.md"
    if readme_file.exists():
        try:
            with open(readme_file, 'r', encoding='utf-8') as f:
                readme_content = f.read()
            
            linhas = readme_content.split('\n')
            print(f"\nüìã RESUMO DO README:")
            print(f"   ‚Ä¢ Total de linhas: {len(linhas)}")
            
            # Extrair t√≠tulos (linhas que come√ßam com #)
            titulos = [linha.strip() for linha in linhas if linha.strip().startswith('#')]
            if titulos:
                print(f"   ‚Ä¢ Se√ß√µes identificadas:")
                for titulo in titulos[:10]:  # Primeiros 10 t√≠tulos
                    print(f"     - {titulo}")
            
            # Procurar por informa√ß√µes importantes
            palavras_chave = ['dengue', 'prediction', 'machine learning', 'model', 'dataset']
            for palavra in palavras_chave:
                if palavra.lower() in readme_content.lower():
                    print(f"   ‚úÖ Menciona '{palavra}'")
                    
        except Exception as e:
            print(f"   ‚ùå Erro ao ler README: {e}")
    else:
        print(f"   ‚ùå README.md n√£o encontrado")
    
    # 3. AN√ÅLISE DOS REQUIREMENTS
    print(f"\n{'-'*60}")
    print("3. DEPEND√äNCIAS (requirements.txt)")
    print(f"{'-'*60}")
    
    req_file = projeto_dir / "requirements.txt"
    if req_file.exists():
        try:
            with open(req_file, 'r', encoding='utf-8') as f:
                requirements = f.read().strip().split('\n')
            
            requirements = [req.strip() for req in requirements if req.strip()]
            
            print(f"\nüì¶ BIBLIOTECAS NECESS√ÅRIAS ({len(requirements)}):")
            
            # Categorizar bibliotecas
            ml_libs = []
            data_libs = []
            viz_libs = []
            other_libs = []
            
            for req in requirements:
                req_lower = req.lower()
                if any(ml in req_lower for ml in ['sklearn', 'xgboost', 'lightgbm', 'catboost', 'tensorflow', 'torch']):
                    ml_libs.append(req)
                elif any(data in req_lower for data in ['pandas', 'numpy', 'scipy']):
                    data_libs.append(req)
                elif any(viz in req_lower for viz in ['matplotlib', 'seaborn', 'plotly']):
                    viz_libs.append(req)
                else:
                    other_libs.append(req)
            
            if ml_libs:
                print(f"   ü§ñ Machine Learning: {ml_libs}")
            if data_libs:
                print(f"   üìä Manipula√ß√£o de dados: {data_libs}")
            if viz_libs:
                print(f"   üìà Visualiza√ß√£o: {viz_libs}")
            if other_libs:
                print(f"   üîß Outras: {other_libs}")
                
        except Exception as e:
            print(f"   ‚ùå Erro ao ler requirements: {e}")
    else:
        print(f"   ‚ùå requirements.txt n√£o encontrado")
    
    return projeto_dir

def analisar_arquivo_python(arquivo_path, nome_arquivo):
    """
    Analisa um arquivo Python espec√≠fico extraindo fun√ß√µes, classes e imports
    """
    print(f"\nüìÑ AN√ÅLISE: {nome_arquivo}")
    print(f"   {'‚îÄ' * (len(nome_arquivo) + 10)}")
    
    try:
        with open(arquivo_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Contar linhas
        linhas = content.split('\n')
        linhas_codigo = [l for l in linhas if l.strip() and not l.strip().startswith('#')]
        
        print(f"   ‚Ä¢ Total de linhas: {len(linhas)}")
        print(f"   ‚Ä¢ Linhas de c√≥digo: {len(linhas_codigo)}")
        
        # Extrair imports
        imports = []
        for linha in linhas:
            linha_strip = linha.strip()
            if linha_strip.startswith('import ') or linha_strip.startswith('from '):
                imports.append(linha_strip)
        
        if imports:
            print(f"   ‚Ä¢ Imports principais:")
            for imp in imports[:5]:  # Primeiros 5 imports
                print(f"     - {imp}")
            if len(imports) > 5:
                print(f"     ... e mais {len(imports) - 5} imports")
        
        # Extrair fun√ß√µes usando regex
        funcoes = re.findall(r'def\s+(\w+)\s*\(([^)]*)\)', content)
        if funcoes:
            print(f"   ‚Ä¢ Fun√ß√µes definidas ({len(funcoes)}):")
            for nome, params in funcoes[:8]:  # Primeiras 8 fun√ß√µes
                params_clean = params.replace('\n', '').replace(' ', '')[:30]
                print(f"     - {nome}({params_clean}{'...' if len(params) > 30 else ''})")
            if len(funcoes) > 8:
                print(f"     ... e mais {len(funcoes) - 8} fun√ß√µes")
        
        # Extrair classes
        classes = re.findall(r'class\s+(\w+).*?:', content)
        if classes:
            print(f"   ‚Ä¢ Classes definidas: {classes}")
        
        # Buscar por algoritmos espec√≠ficos de ML
        ml_algorithms = [
            'RandomForest', 'XGBoost', 'LightGBM', 'CatBoost', 
            'LinearRegression', 'SVM', 'SVR', 'ExtraTrees',
            'SARIMA', 'ARIMA', 'Neural', 'KNN'
        ]
        
        algoritmos_encontrados = []
        for algo in ml_algorithms:
            if algo.lower() in content.lower():
                algoritmos_encontrados.append(algo)
        
        if algoritmos_encontrados:
            print(f"   ü§ñ Algoritmos ML identificados: {algoritmos_encontrados}")
        
        # Identificar prop√≥sito do arquivo baseado no nome e conte√∫do
        if 'main' in nome_arquivo.lower():
            print(f"   üéØ PROP√ìSITO: Arquivo principal de execu√ß√£o")
        elif 'method' in nome_arquivo.lower():
            print(f"   üéØ PROP√ìSITO: Implementa√ß√£o de m√©todos/algoritmos")
        elif 'measure' in nome_arquivo.lower() or 'metric' in nome_arquivo.lower():
            print(f"   üéØ PROP√ìSITO: M√©tricas e avalia√ß√£o de modelos")
        elif 'support' in nome_arquivo.lower():
            print(f"   üéØ PROP√ìSITO: Fun√ß√µes de suporte/utilit√°rios")
        elif 'neighbor' in nome_arquivo.lower():
            print(f"   üéØ PROP√ìSITO: M√©todos baseados em vizinhan√ßa (KNN, etc)")
        
    except Exception as e:
        print(f"   ‚ùå Erro ao analisar arquivo: {e}")

def gerar_relatorio_completo():
    """
    Gera relat√≥rio completo do projeto
    """
    
    # An√°lise inicial
    projeto_dir = analisar_projeto_fabio_kon()
    
    if not projeto_dir:
        return
    
    # 4. AN√ÅLISE DETALHADA DOS ARQUIVOS PYTHON
    print(f"\n{'-'*60}")
    print("4. AN√ÅLISE DETALHADA DOS C√ìDIGOS")
    print(f"{'-'*60}")
    
    arquivos_python = sorted(projeto_dir.glob("*.py"))
    
    # Ordem de an√°lise por import√¢ncia
    ordem_analise = [
        'main.py',
        'mlMethods.py', 
        'measuringResults.py',
        'mlSupportMethods.py',
        'neighbors_methods.py',
        'regression_metrics.py',
        'apply_grid_search_sarima.py',
        'analise_projeto.py'
    ]
    
    # Analisar na ordem de import√¢ncia
    for nome_arquivo in ordem_analise:
        arquivo_path = projeto_dir / nome_arquivo
        if arquivo_path.exists():
            analisar_arquivo_python(arquivo_path, nome_arquivo)
    
    # Analisar arquivos restantes
    for arquivo in arquivos_python:
        if arquivo.name not in ordem_analise:
            analisar_arquivo_python(arquivo, arquivo.name)
    
    # 5. AN√ÅLISE DO NOTEBOOK JUPYTER
    print(f"\n{'-'*60}")
    print("5. NOTEBOOK JUPYTER")
    print(f"{'-'*60}")
    
    notebook_file = projeto_dir / "Shap_value_force_plot.ipynb"
    if notebook_file.exists():
        tamanho = notebook_file.stat().st_size / 1024
        print(f"\nüìì {notebook_file.name} ({tamanho:.1f} KB)")
        print(f"   üéØ PROP√ìSITO: An√°lise de explicabilidade com SHAP values")
        print(f"   üîç FUNCIONALIDADE: Gr√°ficos de for√ßa para interpreta√ß√£o de modelos")
    
    # 6. CONCLUS√ïES E CAPACIDADES
    print(f"\n{'-'*60}")
    print("6. RESUMO DAS CAPACIDADES DO PROJETO")
    print(f"{'-'*60}")
    
    print(f"\nüéØ OBJETIVO PRINCIPAL:")
    print(f"   Predi√ß√£o de casos de dengue usando m√∫ltiplos algoritmos de ML")
    
    print(f"\nü§ñ ALGORITMOS IMPLEMENTADOS:")
    print(f"   ‚Ä¢ Regress√£o Linear (simples e m√∫ltipla)")
    print(f"   ‚Ä¢ Regress√£o Polinomial") 
    print(f"   ‚Ä¢ Random Forest")
    print(f"   ‚Ä¢ Extra Trees")
    print(f"   ‚Ä¢ Support Vector Regression (SVR)")
    print(f"   ‚Ä¢ XGBoost")
    print(f"   ‚Ä¢ LightGBM")
    print(f"   ‚Ä¢ CatBoost")
    print(f"   ‚Ä¢ M√©todos baseados em vizinhan√ßa (KNN)")
    print(f"   ‚Ä¢ SARIMA (s√©ries temporais)")
    
    print(f"\n‚öôÔ∏è FUNCIONALIDADES:")
    print(f"   ‚Ä¢ Grid Search autom√°tico para otimiza√ß√£o")
    print(f"   ‚Ä¢ Cross-validation para avalia√ß√£o")
    print(f"   ‚Ä¢ M√∫ltiplas m√©tricas de regress√£o")
    print(f"   ‚Ä¢ Feature scaling e sele√ß√£o")
    print(f"   ‚Ä¢ Backward elimination")
    print(f"   ‚Ä¢ Explicabilidade com SHAP")
    
    print(f"\n‚úÖ PONTOS FORTES:")
    print(f"   ‚Ä¢ Implementa√ß√£o completa de pipeline de ML")
    print(f"   ‚Ä¢ M√∫ltiplos algoritmos prontos para uso")
    print(f"   ‚Ä¢ Otimiza√ß√£o autom√°tica de hiperpar√¢metros")
    print(f"   ‚Ä¢ Sistema modular e reutiliz√°vel")
    print(f"   ‚Ä¢ M√©tricas de avalia√ß√£o abrangentes")
    
    print(f"\nüí° APLICABILIDADE PARA SEU MESTRADO:")
    print(f"   üéØ IDEAL para seu projeto de predi√ß√£o de dengue!")
    print(f"   üìä Pode usar os dados dengue_poa + MiAedes como entrada")
    print(f"   üîß J√° tem tudo implementado, s√≥ adaptar os dados")
    print(f"   üìà Pode comparar todos os algoritmos automaticamente")
    print(f"   üìù Base s√≥lida para metodologia da disserta√ß√£o")

if __name__ == "__main__":
    gerar_relatorio_completo()